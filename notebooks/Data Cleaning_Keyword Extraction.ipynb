{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c150ef-e7f2-4028-90b1-6c09b1c61a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_upwork_jobs_2024-02-07-2024-03-24 (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c290fee5-a372-46e2-b047-036d3a52ead7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data...\n",
      " NLTK data downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"Downloading NLTK data...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "print(\" NLTK data downloaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d27b4433-5593-4beb-94c0-0ba3a3d08836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully: 244828 rows, 8 columns\n",
      "\n",
      "==================================================\n",
      "DATASET OVERVIEW\n",
      "==================================================\n",
      "Shape: (244828, 8)\n",
      "Columns: ['title', 'link', 'published_date', 'is_hourly', 'hourly_low', 'hourly_high', 'budget', 'country']\n",
      "Memory usage: 93.19 MB\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('all_upwork_jobs_2024-02-07-2024-03-24 (2).csv')  \n",
    "print(f\"Data loaded successfully: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1164a6c-34ec-4b01-8159-8a7907d373a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MISSING VALUES ANALYSIS\n",
      "==================================================\n",
      "           Column  Missing Count  Missing %\n",
      "0           title              1   0.000408\n",
      "1            link              1   0.000408\n",
      "2  published_date              0   0.000000\n",
      "3       is_hourly              0   0.000000\n",
      "4      hourly_low         142406  58.165733\n",
      "5     hourly_high         146053  59.655350\n",
      "6          budget         140937  57.565720\n",
      "7         country           5077   2.073701\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_data.index,\n",
    "    'Missing Count': missing_data.values,\n",
    "    'Missing %': missing_percent.values\n",
    "})\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8011763-a82a-4b90-894b-d7d3661ff6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DATA TYPES\n",
      "==================================================\n",
      "title              object\n",
      "link               object\n",
      "published_date     object\n",
      "is_hourly            bool\n",
      "hourly_low        float64\n",
      "hourly_high       float64\n",
      "budget            float64\n",
      "country            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Data types\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\"*50)\n",
    "print(df.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ee233cd-2244-44fe-8244-6b8f0e2de05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STARTING DATA CLEANING PROCESS\n",
      "==================================================\n",
      "\n",
      " Step 1: Cleaning date columns...\n",
      " Date cleaning completed\n",
      "\n",
      " Step 2: Cleaning numeric columns...\n",
      " Numeric data cleaning completed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING DATA CLEANING PROCESS\")\n",
    "print(\"=\"*50)\n",
    "# Step 1: Date cleaning\n",
    "print(\"\\n Step 1: Cleaning date columns...\")\n",
    "df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
    "df['year'] = df['published_date'].dt.year\n",
    "df['month'] = df['published_date'].dt.month\n",
    "df['day_of_week'] = df['published_date'].dt.day_name()\n",
    "df['hour'] = df['published_date'].dt.hour\n",
    "df['day_of_month'] = df['published_date'].dt.day\n",
    "print(\" Date cleaning completed\")\n",
    "\n",
    "# Step 2: Numeric data cleaning\n",
    "print(\"\\n Step 2: Cleaning numeric columns...\")\n",
    "\n",
    "# Clean hourly rates\n",
    "df['hourly_low'] = pd.to_numeric(df['hourly_low'], errors='coerce')\n",
    "df['hourly_high'] = pd.to_numeric(df['hourly_high'], errors='coerce')\n",
    "df['budget'] = pd.to_numeric(df['budget'], errors='coerce')\n",
    "# Calculate average hourly rate\n",
    "df['avg_hourly_rate'] = np.where(\n",
    "    df['is_hourly'] == True,\n",
    "    (df['hourly_low'].fillna(0) + df['hourly_high'].fillna(0)) / 2,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Remove outliers (rates above $200/hour or budgets above $50000)\n",
    "df.loc[df['avg_hourly_rate'] > 200, 'avg_hourly_rate'] = np.nan\n",
    "df.loc[df['budget'] > 50000, 'budget'] = np.nan\n",
    "\n",
    "print(\" Numeric data cleaning completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3d835ff-1efd-49fb-8e38-f27f944fd80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step 3 Text preprocessing and stopword removal...\n",
      " Text preprocessing completed\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Text cleaning and preprocessing\n",
    "print(\"\\n Step 3 Text preprocessing and stopword removal...\")\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Additional stopwords specific to job postings\n",
    "custom_stopwords = {\n",
    "    'job', 'work', 'position', 'role', 'opportunity', 'candidate', \n",
    "    'experience', 'skill', 'requirement', 'looking', 'needed', 'want',\n",
    "    'seeking', 'hiring', 'required', 'preferred', 'must', 'should',\n",
    "    'will', 'able', 'new', 'project', 'company', 'team', 'client',\n",
    "    'service', 'business', 'help', 'get', 'make', 'time', 'good',\n",
    "    'one', 'two', 'first', 'last', 'also', 'well', 'way', 'come',\n",
    "    'go', 'see', 'know', 'take', 'use', 'find', 'give', 'tell',\n",
    "    'ask', 'seem', 'feel', 'try', 'leave', 'call', 'move', 'live',\n",
    "    'believe', 'hold', 'bring', 'happen', 'write', 'sit', 'stand',\n",
    "    'hear', 'let', 'begin', 'seem', 'turn', 'start', 'might', 'show',\n",
    "    'every', 'great', 'small', 'public', 'able'\n",
    "}\n",
    "stop_words.update(custom_stopwords)\n",
    "# Clean job titles\n",
    "df['title_original'] = df['title'].copy()\n",
    "df['title_clean'] = df['title'].str.lower()\n",
    "\n",
    "# Remove special characters and numbers\n",
    "df['title_clean'] = df['title_clean'].str.replace(r'[^a-zA-Z\\s]', ' ', regex=True)\n",
    "df['title_clean'] = df['title_clean'].str.replace(r'\\s+', ' ', regex=True)\n",
    "df['title_clean'] = df['title_clean'].str.strip()\n",
    "\n",
    "# Tokenize and remove stopwords\n",
    "df['title_tokens'] = df['title_clean'].apply(\n",
    "    lambda x: [word for word in word_tokenize(str(x)) if word not in stop_words and len(word) > 2]\n",
    ")\n",
    "\n",
    "# Apply stemming\n",
    "df['title_stemmed'] = df['title_tokens'].apply(\n",
    "    lambda x: [stemmer.stem(word) for word in x]\n",
    ")\n",
    "\n",
    "# Create processed title\n",
    "df['title_processed'] = df['title_stemmed'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "print(\" Text preprocessing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1af97b68-2100-4df3-b5d4-77c3a3a43778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step 4: Discovering job categories from data...\n",
      "Found 73 key terms in job titles\n",
      "\n",
      "Top 20 most frequent terms:\n",
      "  design: 27455 occurrences\n",
      "  develop: 23490 occurrences\n",
      "  websit: 17725 occurrences\n",
      "  video: 16499 occurrences\n",
      "  expert: 15388 occurrences\n",
      "  need: 10681 occurrences\n",
      "  manag: 10586 occurrences\n",
      "  market: 10562 occurrences\n",
      "  amp: 9690 occurrences\n",
      "  specialist: 9148 occurrences\n",
      "  app: 8843 occurrences\n",
      "  editor: 8314 occurrences\n",
      "  assist: 8312 occurrences\n",
      "  media: 8268 occurrences\n",
      "  creat: 7890 occurrences\n",
      "  social: 7718 occurrences\n",
      "  youtub: 7697 occurrences\n",
      "  data: 7163 occurrences\n",
      "  content: 6889 occurrences\n",
      "  wordpress: 6787 occurrences\n",
      "\n",
      "Discovered categories and their key terms:\n",
      "\n",
      "Software Development:\n",
      "  Key terms: full, develop, app, engin, websit, web, mobil, applic\n",
      "  Total terms: 8\n",
      "\n",
      "Design & Creative:\n",
      "  Key terms: design, logo, build, graphic, brand\n",
      "  Total terms: 5\n",
      "\n",
      "Data & Analytics:\n",
      "  Key terms: data, research\n",
      "  Total terms: 2\n",
      "\n",
      "Marketing & Sales:\n",
      "  Key terms: media, market, seo, social, content\n",
      "  Total terms: 5\n",
      "\n",
      "Writing & Content:\n",
      "  Key terms: writer, editor, content, translat\n",
      "  Total terms: 4\n",
      "\n",
      "Customer Support:\n",
      "  Key terms: custom, assist\n",
      "  Total terms: 2\n",
      "\n",
      "Business & Management:\n",
      "  Key terms: manag, lead, product\n",
      "  Total terms: 3\n",
      "\n",
      "Finance & Accounting:\n",
      "  Key terms: account\n",
      "  Total terms: 1\n"
     ]
    }
   ],
   "source": [
    "# Step 4: DATA-DRIVEN CATEGORY DISCOVERY\n",
    "print(\"\\n Step 4: Discovering job categories from data...\")\n",
    "\n",
    "def extract_key_terms_from_titles():\n",
    "    \"\"\"Extract most common meaningful terms from job titles\"\"\"\n",
    "    # Get all processed tokens\n",
    "    all_tokens = []\n",
    "    for tokens in df['title_stemmed']:\n",
    "        all_tokens.extend(tokens)\n",
    "    \n",
    "    # Count frequency of terms\n",
    "    term_counts = Counter(all_tokens)\n",
    "    \n",
    "    # Get most common terms (excluding very rare ones)\n",
    "    min_frequency = max(2, len(df) // 100)  # At least 2 or 1% of dataset\n",
    "    common_terms = {term: count for term, count in term_counts.items() \n",
    "                   if count >= min_frequency}\n",
    "    \n",
    "    return common_terms\n",
    "\n",
    "def create_data_driven_categories():\n",
    "    \"\"\"Create categories based on actual data patterns\"\"\"\n",
    "    \n",
    "    # Extract key terms\n",
    "    key_terms = extract_key_terms_from_titles()\n",
    "    print(f\"Found {len(key_terms)} key terms in job titles\")\n",
    "    \n",
    "    # Display top terms\n",
    "    top_terms = Counter(key_terms).most_common(20)\n",
    "    print(\"\\nTop 20 most frequent terms:\")\n",
    "    for term, count in top_terms:\n",
    "        print(f\"  {term}: {count} occurrences\")\n",
    "    \n",
    "    # Create category mapping based on common patterns\n",
    "    categories = {}\n",
    "    \n",
    "    # Development related terms\n",
    "    dev_terms = [term for term, count in key_terms.items() \n",
    "                if any(keyword in term.lower() for keyword in \n",
    "                      ['develop', 'program', 'cod', 'app', 'web', 'mobil', 'softwar', \n",
    "                       'frontend', 'backend', 'fullstack', 'full', 'stack', 'engin'])]\n",
    "    \n",
    "    # Design related terms\n",
    "    design_terms = [term for term, count in key_terms.items() \n",
    "                   if any(keyword in term.lower() for keyword in \n",
    "                         ['design', 'ui', 'ux', 'graphic', 'logo', 'brand', 'creativ', 'visual'])]\n",
    "    \n",
    "    # Data related terms\n",
    "    data_terms = [term for term, count in key_terms.items() \n",
    "                 if any(keyword in term.lower() for keyword in \n",
    "                       ['data', 'analyst', 'analyt', 'scienc', 'research', 'databas', 'sql'])]\n",
    "    \n",
    "    # Marketing related terms\n",
    "    marketing_terms = [term for term, count in key_terms.items() \n",
    "                      if any(keyword in term.lower() for keyword in \n",
    "                            ['market', 'seo', 'social', 'media', 'advertis', 'campaign', 'content'])]\n",
    "    \n",
    "    # Writing related terms\n",
    "    writing_terms = [term for term, count in key_terms.items() \n",
    "                    if any(keyword in term.lower() for keyword in \n",
    "                          ['writ', 'content', 'blog', 'copywr', 'editor', 'translat'])]\n",
    "    \n",
    "    # Support related terms\n",
    "    support_terms = [term for term, count in key_terms.items() \n",
    "                    if any(keyword in term.lower() for keyword in \n",
    "                          ['support', 'custom', 'servic', 'help', 'assist'])]\n",
    "    \n",
    "    # Management related terms\n",
    "    mgmt_terms = [term for term, count in key_terms.items() \n",
    "                 if any(keyword in term.lower() for keyword in \n",
    "                       ['manag', 'project', 'product', 'coordinat', 'lead', 'director'])]\n",
    "    \n",
    "    # Finance related terms\n",
    "    finance_terms = [term for term, count in key_terms.items() \n",
    "                    if any(keyword in term.lower() for keyword in \n",
    "                          ['financ', 'account', 'bookkeep', 'budget', 'tax', 'payrol'])]\n",
    "    \n",
    "    # Store discovered categories\n",
    "    discovered_categories = {\n",
    "        'Software Development': dev_terms,\n",
    "        'Design & Creative': design_terms,\n",
    "        'Data & Analytics': data_terms,\n",
    "        'Marketing & Sales': marketing_terms,\n",
    "        'Writing & Content': writing_terms,\n",
    "        'Customer Support': support_terms,\n",
    "        'Business & Management': mgmt_terms,\n",
    "        'Finance & Accounting': finance_terms\n",
    "    }\n",
    "    \n",
    "    print(\"\\nDiscovered categories and their key terms:\")\n",
    "    for category, terms in discovered_categories.items():\n",
    "        if terms:  # Only show categories with terms\n",
    "            print(f\"\\n{category}:\")\n",
    "            print(f\"  Key terms: {', '.join(terms[:10])}\")  # Show first 10 terms\n",
    "            print(f\"  Total terms: {len(terms)}\")\n",
    "    \n",
    "    return discovered_categories, key_terms\n",
    "\n",
    "# Discover categories from data\n",
    "discovered_categories, all_key_terms = create_data_driven_categories()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3116e43-c377-4905-b248-495487500e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step 5: Applying data-driven job categorization...\n",
      "\n",
      "First pass: 93258 jobs remain uncategorized\n",
      "\n",
      "Analyzing uncategorized jobs for patterns:\n",
      "   1. Want to fix the WordPress Plugin\n",
      "   2. URGENT: Fix Emails Not Working on Discourse Installation\n",
      "   3. Shopify Store speed Optimization\n",
      "   4. Promo Video for Game\n",
      "   5. Report Analysis\n",
      "   6. I'm looking for a person who knows how to write algorithms in the Chrome browser extension\n",
      "   7. Linkedin Ads Coach\n",
      "   8. Shopify Shop Implementation\n",
      "   9. CapCut genius\n",
      "  10. Convert single page figma to Nextjs - Urgently\n",
      "After second pass: 70026 jobs remain as 'Other'\n",
      " Enhanced job categorization completed\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Apply data-driven categorization\n",
    "print(\"\\n Step 5: Applying data-driven job categorization...\")\n",
    "\n",
    "def categorize_job_title(processed_title, categories):\n",
    "    \"\"\"Categorize a job title based on discovered patterns\"\"\"\n",
    "    title_lower = processed_title.lower()\n",
    "    \n",
    "    # Score each category\n",
    "    category_scores = {}\n",
    "    for category, terms in categories.items():\n",
    "        score = sum(1 for term in terms if term in title_lower)\n",
    "        if score > 0:\n",
    "            category_scores[category] = score\n",
    "    \n",
    "    # Return category with highest score, or 'Other' if no match\n",
    "    if category_scores:\n",
    "        return max(category_scores, key=category_scores.get)\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply enhanced categorization\n",
    "df['job_category'] = 'Other'  # Initialize\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if pd.notna(row['title_processed']) and len(str(row['title_processed']).strip()) > 0:\n",
    "        df.loc[idx, 'job_category'] = categorize_job_title(row['title_processed'], discovered_categories)\n",
    "\n",
    "# Additional pass: Check for uncategorized jobs and try alternative matching\n",
    "uncategorized = df[df['job_category'] == 'Other']\n",
    "print(f\"\\nFirst pass: {len(uncategorized)} jobs remain uncategorized\")\n",
    "\n",
    "if len(uncategorized) > 0:\n",
    "    print(\"\\nAnalyzing uncategorized jobs for patterns:\")\n",
    "    sample_uncategorized = uncategorized['title_original'].head(10).tolist()\n",
    "    for i, title in enumerate(sample_uncategorized, 1):\n",
    "        print(f\"  {i:2d}. {title}\")\n",
    "    \n",
    "    # Try to capture more with relaxed criteria\n",
    "    for idx, row in uncategorized.iterrows():\n",
    "        title_orig = str(row['title_original']).lower()\n",
    "        \n",
    "        # Manual rules for common patterns not caught\n",
    "        if any(word in title_orig for word in ['media', 'buyer', 'ads', 'campaign', 'marketing', 'seo', 'social']):\n",
    "            df.loc[idx, 'job_category'] = 'Marketing & Advertising'\n",
    "        elif any(word in title_orig for word in ['app', 'web', 'development', 'developer', 'programming', 'coding']):\n",
    "            df.loc[idx, 'job_category'] = 'Software Development'\n",
    "        elif any(word in title_orig for word in ['design', 'designer', 'creative', 'visual', 'logo', '3d', 'graphic']):\n",
    "            df.loc[idx, 'job_category'] = 'Design & Creative'\n",
    "        elif any(word in title_orig for word in ['writer', 'writing', 'content', 'translation', 'portuguese', 'blog']):\n",
    "            df.loc[idx, 'job_category'] = 'Writing & Content'\n",
    "        elif any(word in title_orig for word in ['data', 'analyst', 'dashboard', 'report', 'looker', 'studio']):\n",
    "            df.loc[idx, 'job_category'] = 'Data & Analytics'\n",
    "        elif any(word in title_orig for word in ['support', 'customer', 'service', 'help', 'chat']):\n",
    "            df.loc[idx, 'job_category'] = 'Customer Support'\n",
    "        elif any(word in title_orig for word in ['manager', 'management', 'coordinator', 'assistant', 'executive']):\n",
    "            df.loc[idx, 'job_category'] = 'Business & Management'\n",
    "        elif any(word in title_orig for word in ['sales', 'hunter', 'talent', 'recruit', 'business development']):\n",
    "            df.loc[idx, 'job_category'] = 'Sales & Business Development'\n",
    "        elif any(word in title_orig for word in ['store', 'shop', 'ecommerce', 'shopify', 'product', 'optimization']):\n",
    "            df.loc[idx, 'job_category'] = 'E-commerce & Retail'\n",
    "\n",
    "final_uncategorized = len(df[df['job_category'] == 'Other'])\n",
    "print(f\"After second pass: {final_uncategorized} jobs remain as 'Other'\")\n",
    "\n",
    "print(\" Enhanced job categorization completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77e03083-219c-4de4-bd89-5369284b2354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step 6: Creating specific subcategories using text clustering...\n",
      " Created 15 specific subcategories using clustering\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Create specific subcategories using clustering\n",
    "print(\"\\n Step 6: Creating specific subcategories using text clustering...\")\n",
    "\n",
    "# Use TF-IDF to create more specific categories\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=100,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "# Only process non-empty titles\n",
    "valid_titles = df[df['title_processed'].str.len() > 0]['title_processed']\n",
    "\n",
    "if len(valid_titles) > 10:  # Only if we have enough data\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(valid_titles)\n",
    "        \n",
    "        # Determine optimal number of clusters (max 15 or 10% of data)\n",
    "        n_clusters = min(15, max(3, len(valid_titles) // 10))\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        clusters = kmeans.fit_predict(tfidf_matrix)\n",
    "        \n",
    "        # Create cluster labels\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        cluster_labels = []\n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            # Get top terms for this cluster\n",
    "            cluster_center = kmeans.cluster_centers_[i]\n",
    "            top_indices = cluster_center.argsort()[-3:][::-1]  # Top 3 terms\n",
    "            top_terms = [feature_names[idx] for idx in top_indices]\n",
    "            \n",
    "            # Create readable label\n",
    "            label = ' '.join(top_terms).title()\n",
    "            cluster_labels.append(f\"Cluster_{i+1}_{label.replace(' ', '_')}\")\n",
    "        \n",
    "        # Apply cluster labels to valid titles\n",
    "        df['specific_category'] = 'Uncategorized'\n",
    "        valid_indices = df[df['title_processed'].str.len() > 0].index\n",
    "        \n",
    "        for idx, cluster_id in zip(valid_indices, clusters):\n",
    "            df.loc[idx, 'specific_category'] = cluster_labels[cluster_id]\n",
    "            \n",
    "        print(f\" Created {n_clusters} specific subcategories using clustering\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Clustering failed: {e}\")\n",
    "        df['specific_category'] = df['job_category']  # Fallback to broad categories\n",
    "else:\n",
    "    df['specific_category'] = df['job_category']  # Fallback to broad categories\n",
    "    print(\"  Too few valid titles for clustering, using broad categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ea57dfa-4ef4-43e0-9cb5-5f6b6b35f47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step 7: Cleaning country data...\n",
      "\n",
      "Countries in dataset:\n",
      "  United States: 99797 jobs\n",
      "  United Kingdom: 19129 jobs\n",
      "  India: 15825 jobs\n",
      "  Australia: 12617 jobs\n",
      "  Canada: 11655 jobs\n",
      "  Pakistan: 5289 jobs\n",
      "  Germany: 4838 jobs\n",
      "  Netherlands: 4435 jobs\n",
      "  United Arab Emirates: 4038 jobs\n",
      "  France: 3161 jobs\n",
      " Country data cleaning completed\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Country data cleaning\n",
    "print(\"\\n Step 7: Cleaning country data...\")\n",
    "\n",
    "# Standardize country names based on actual data\n",
    "country_counts = df['country'].value_counts()\n",
    "print(\"\\nCountries in dataset:\")\n",
    "for country, count in country_counts.head(10).items():\n",
    "    print(f\"  {country}: {count} jobs\")\n",
    "\n",
    "# Simple country cleaning (you can extend this based on your data)\n",
    "country_mapping = {\n",
    "    'United States': 'USA',\n",
    "    'United Kingdom': 'UK',\n",
    "    'Deutschland': 'Germany',\n",
    "    'Brasil': 'Brazil',\n",
    "    'EspaÃ±a': 'Spain',\n",
    "    'United Arab Emirates':'UAE'\n",
    "}\n",
    "\n",
    "df['country_clean'] = df['country'].replace(country_mapping)\n",
    "df['country_clean'] = df['country_clean'].fillna('Unknown')\n",
    "\n",
    "print(\" Country data cleaning completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73aedec5-7c9b-45e0-a93d-b886d3ce3fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step 8: Creating additional features...\n",
      " Feature engineering completed\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Create additional features\n",
    "print(\"\\n Step 8: Creating additional features...\")\n",
    "\n",
    "# Is weekend posting\n",
    "df['is_weekend'] = df['day_of_week'].isin(['Saturday', 'Sunday'])\n",
    "\n",
    "# Is evening posting (after 6 PM)\n",
    "df['is_evening_post'] = df['hour'] >= 18\n",
    "\n",
    "# Title length features\n",
    "df['title_length'] = df['title_original'].str.len()\n",
    "df['title_word_count'] = df['title_original'].str.split().str.len()\n",
    "\n",
    "# Has specific keywords (discovered from data)\n",
    "top_keywords = Counter(all_key_terms).most_common(20)\n",
    "urgent_keywords = [kw for kw, count in top_keywords if 'urgent' in kw.lower()]\n",
    "remote_keywords = [kw for kw, count in top_keywords if any(term in kw.lower() for term in ['remote', 'home'])]\n",
    "senior_keywords = [kw for kw, count in top_keywords if any(term in kw.lower() for term in ['senior', 'lead', 'principal'])]\n",
    "junior_keywords = [kw for kw, count in top_keywords if any(term in kw.lower() for term in ['junior', 'entry', 'intern'])]\n",
    "\n",
    "df['has_urgent'] = df['title_original'].str.contains('urgent|asap|immediate', case=False, na=False)\n",
    "df['has_remote'] = df['title_original'].str.contains('remote|work from home|wfh', case=False, na=False)\n",
    "df['has_senior'] = df['title_original'].str.contains('senior|sr\\.|lead|principal', case=False, na=False)\n",
    "df['has_junior'] = df['title_original'].str.contains('junior|jr\\.|entry|intern', case=False, na=False)\n",
    "\n",
    "print(\" Feature engineering completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bef7c6a1-d85d-436d-9855-e271c3aa5ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Step 9: Final data validation...\n",
      "Removed 0 rows with critical missing data\n",
      "Final dataset size: 244656 rows\n",
      "\n",
      "============================================================\n",
      "DATA-DRIVEN CLEANING SUMMARY\n",
      "============================================================\n",
      " Original rows: 244656\n",
      " Final rows: 244656\n",
      " Total unique terms discovered: 73\n",
      " Job categories created: 12\n",
      " Specific subcategories: 15\n",
      " Countries found: 213\n",
      " Date range: 2023-11-02 09:22:02+00:00 to 2024-03-24 14:16:47+00:00\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Final data validation\n",
    "print(\"\\n Step 9: Final data validation...\")\n",
    "\n",
    "# Remove rows with critical missing data\n",
    "initial_rows = len(df)\n",
    "df = df.dropna(subset=['title_processed', 'published_date'])\n",
    "df = df[df['title_processed'].str.len() > 0]  # Remove empty processed titles\n",
    "\n",
    "print(f\"Removed {initial_rows - len(df)} rows with critical missing data\")\n",
    "print(f\"Final dataset size: {len(df)} rows\")\n",
    "\n",
    "# Data quality summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA-DRIVEN CLEANING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\" Original rows: {initial_rows}\")\n",
    "print(f\" Final rows: {len(df)}\")\n",
    "print(f\" Total unique terms discovered: {len(all_key_terms)}\")\n",
    "print(f\" Job categories created: {df['job_category'].nunique()}\")\n",
    "print(f\" Specific subcategories: {df['specific_category'].nunique()}\")\n",
    "print(f\" Countries found: {df['country_clean'].nunique()}\")\n",
    "print(f\" Date range: {df['published_date'].min()} to {df['published_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b578b7b1-debf-4c4b-b9c7-5434b724d651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cleaned data saved to: cleaned_job_data_driven.csv\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned data\n",
    "output_file = 'cleaned_job_data_driven.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\n Cleaned data saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38d37232-6538-40ab-9f8d-d8f4c3605a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA-DRIVEN JOB CATEGORIES SUMMARY\n",
      "============================================================\n",
      "Broad Categories (discovered from data):\n",
      "  Other                      70026 jobs ( 28.6%)\n",
      "  Software Development       56842 jobs ( 23.2%)\n",
      "  Design & Creative          31152 jobs ( 12.7%)\n",
      "  Marketing & Sales          20286 jobs (  8.3%)\n",
      "  Writing & Content          18035 jobs (  7.4%)\n",
      "  Customer Support           13278 jobs (  5.4%)\n",
      "  Business & Management      11539 jobs (  4.7%)\n",
      "  Data & Analytics           10117 jobs (  4.1%)\n",
      "  Marketing & Advertising     3798 jobs (  1.6%)\n",
      "  Sales & Business Development   3452 jobs (  1.4%)\n",
      "  E-commerce & Retail         3377 jobs (  1.4%)\n",
      "  Finance & Accounting        2754 jobs (  1.1%)\n",
      "\n",
      "Specific Subcategories (Top 10):\n",
      "  Cluster_1_Assist_Need_Data          128400 jobs ( 52.5%)\n",
      "  Cluster_11_Develop_Stack_Web         14075 jobs (  5.8%)\n",
      "  Cluster_15_Websit_Develop_Design     12530 jobs (  5.1%)\n",
      "  Cluster_4_Expert_Googl_Seo           12279 jobs (  5.0%)\n",
      "  Cluster_8_Design_Web_Product         11481 jobs (  4.7%)\n",
      "  Cluster_6_Specialist_Market_Seo       8351 jobs (  3.4%)\n",
      "  Cluster_12_App_Develop_Mobil          7913 jobs (  3.2%)\n",
      "  Cluster_14_Media_Social_Social_Media   7370 jobs (  3.0%)\n",
      "  Cluster_3_Manag_Market_Ad             7347 jobs (  3.0%)\n",
      "  Cluster_5_Editor_Video_Editor_Video   7241 jobs (  3.0%)\n"
     ]
    }
   ],
   "source": [
    "# Display discovered categories summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA-DRIVEN JOB CATEGORIES SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "category_summary = df['job_category'].value_counts()\n",
    "print(\"Broad Categories (discovered from data):\")\n",
    "for category, count in category_summary.items():\n",
    "    percentage = count / len(df) * 100\n",
    "    print(f\"  {category:<25} {count:>6} jobs ({percentage:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nSpecific Subcategories (Top 10):\")\n",
    "specific_summary = df['specific_category'].value_counts().head(10)\n",
    "for category, count in specific_summary.items():\n",
    "    percentage = count / len(df) * 100\n",
    "    print(f\"  {category:<35} {count:>6} jobs ({percentage:>5.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5623af0c-8e5b-4ce7-932b-149710f6cc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOP TERMS BY DISCOVERED CATEGORY\n",
      "============================================================\n",
      "\n",
      "Software Development (56842 jobs):\n",
      "  Top terms: develop(23490), websit(17725), app(8843), web(6764), engin(3657)\n",
      "\n",
      "Design & Creative (31152 jobs):\n",
      "  Top terms: design(27455), graphic(6290), logo(5869), build(5508), brand(4610)\n",
      "\n",
      "Data & Analytics (10117 jobs):\n",
      "  Top terms: data(7163), research(3723)\n",
      "\n",
      "Marketing & Sales (20286 jobs):\n",
      "  Top terms: market(10562), media(8268), social(7718), content(6889), seo(5084)\n",
      "\n",
      "Writing & Content (18035 jobs):\n",
      "  Top terms: editor(8314), content(6889), writer(5625), translat(3933)\n",
      "\n",
      "Customer Support (13278 jobs):\n",
      "  Top terms: assist(8312), custom(4453)\n",
      "\n",
      "Business & Management (11539 jobs):\n",
      "  Top terms: manag(10586), product(6707), lead(4800)\n",
      "\n",
      "Finance & Accounting (2754 jobs):\n",
      "  Top terms: account(4279)\n"
     ]
    }
   ],
   "source": [
    "# Show most common terms by category\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP TERMS BY DISCOVERED CATEGORY\")\n",
    "print(\"=\"*60)\n",
    "for category, terms in discovered_categories.items():\n",
    "    if terms and category in df['job_category'].values:\n",
    "        job_count = len(df[df['job_category'] == category])\n",
    "        print(f\"\\n{category} ({job_count} jobs):\")\n",
    "        # Show terms sorted by frequency\n",
    "        category_terms = [(term, all_key_terms[term]) for term in terms[:10]]\n",
    "        category_terms.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"  Top terms: {', '.join([f'{term}({count})' for term, count in category_terms[:5]])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc4274d2-e5a7-44c7-96c5-a8be8d7a93df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAMPLE OF CLEANED DATA\n",
      "============================================================\n",
      "                                      title_original  \\\n",
      "0  Experienced Media Buyer For Solar Pannel and R...   \n",
      "1                               Full Stack Developer   \n",
      "2                                    SMMA Bubble App   \n",
      "3             Talent Hunter Specialized in Marketing   \n",
      "4                                      Data Engineer   \n",
      "5               SEO for Portuguese Psychologist site   \n",
      "6                   Want to fix the WordPress Plugin   \n",
      "7  need Portuguese writers who can understand and...   \n",
      "8  Looker Studio Dashboard for Leadgen and E-Comm...   \n",
      "9  PHP/HTML/CSS WordPress Developer Needed for We...   \n",
      "\n",
      "                                     title_processed           job_category  \\\n",
      "0  experienc media buyer solar pannel roof instal...      Marketing & Sales   \n",
      "1                                 full stack develop   Software Development   \n",
      "2                                     smma bubbl app   Software Development   \n",
      "3                       talent hunter special market      Marketing & Sales   \n",
      "4                                         data engin   Software Development   \n",
      "5                    seo portugues psychologist site      Marketing & Sales   \n",
      "6                               fix wordpress plugin                  Other   \n",
      "7         need portugues writer understand portugues      Writing & Content   \n",
      "8  looker studio dashboard leadgen commerc tool q...  Business & Management   \n",
      "9    php html css wordpress develop websit transform   Software Development   \n",
      "\n",
      "                    specific_category country_clean  avg_hourly_rate  budget  \\\n",
      "0          Cluster_1_Assist_Need_Data       Unknown              NaN   500.0   \n",
      "1        Cluster_11_Develop_Stack_Web           USA              NaN  1100.0   \n",
      "2        Cluster_12_App_Develop_Mobil           USA             20.0     NaN   \n",
      "3          Cluster_1_Assist_Need_Data           USA              0.0     NaN   \n",
      "4          Cluster_1_Assist_Need_Data         India              NaN   650.0   \n",
      "5          Cluster_1_Assist_Need_Data      Portugal              0.0     NaN   \n",
      "6  Cluster_2_Wordpress_Websit_Develop         India              NaN     5.0   \n",
      "7          Cluster_1_Assist_Need_Data         India             14.5     NaN   \n",
      "8          Cluster_1_Assist_Need_Data       Germany              0.0     NaN   \n",
      "9  Cluster_2_Wordpress_Websit_Develop        Canada              NaN   500.0   \n",
      "\n",
      "   is_hourly  year  month  \n",
      "0      False  2024      2  \n",
      "1      False  2024      2  \n",
      "2       True  2024      2  \n",
      "3       True  2024      2  \n",
      "4      False  2024      2  \n",
      "5       True  2024      2  \n",
      "6      False  2024      2  \n",
      "7       True  2024      2  \n",
      "8       True  2024      2  \n",
      "9      False  2024      2  \n",
      "\n",
      " Data-driven cleaning completed successfully!\n",
      "Next step: Run your EDA analysis with: cleaned_job_data_driven.csv\n",
      "\n",
      "Key improvements in this data-driven approach:\n",
      "1. Categories discovered from actual job titles in your dataset\n",
      "2. Term frequency analysis to identify most relevant keywords\n",
      "3.  Automatic clustering for specific subcategories\n",
      "4. Adaptive to your specific dataset characteristics\n"
     ]
    }
   ],
   "source": [
    "# Display sample of cleaned data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE OF CLEANED DATA\")\n",
    "print(\"=\"*60)\n",
    "sample_columns = ['title_original', 'title_processed', 'job_category', 'specific_category', \n",
    "                 'country_clean', 'avg_hourly_rate', 'budget', 'is_hourly', 'year', 'month']\n",
    "print(df[sample_columns].head(10))\n",
    "\n",
    "print(\"\\n Data-driven cleaning completed successfully!\")\n",
    "print(f\"Next step: Run your EDA analysis with: {output_file}\")\n",
    "print(\"\\nKey improvements in this data-driven approach:\")\n",
    "print(\"1. Categories discovered from actual job titles in your dataset\")\n",
    "print(\"2. Term frequency analysis to identify most relevant keywords\")\n",
    "print(\"3.  Automatic clustering for specific subcategories\")\n",
    "print(\"4. Adaptive to your specific dataset characteristics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef576bc-41ac-4552-aa71-1df82c9d8b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
